{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nkululeko-VillI-Nhlapo/Machine-Learning-with-Python/blob/main/sms_text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbRjjFi4SFIX"
      },
      "source": [
        "# Neural Network SMS Text Classifier\n",
        "### Nhlapo Nkululeko\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvUf-LRfSFIb"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "Welcome to the SMS Spam Classification project, designed to detect and classify spam messages using machine learning techniques. This project is a culmination of efforts to enhance communication security and efficiency through advanced data analysis and modeling techniques.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   The primary objective of this project was to develop a robust machine learning model capable of accurately distinguishing between \"ham\" (legitimate, normal) and \"spam\" messages in SMS communication.\n",
        "*     Utilizing the SMS Spam Collection dataset provided by FreeCodeCamp, This project is part of the FreeCodeCamp Machine Learning certification.\n",
        "\n",
        "**Scroll down and Explore, it is worth the Time.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUrEstsgSFIc"
      },
      "source": [
        "Okay Let us Begin Now, we will start by importing all the necessary libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOaKYeiSSFIe",
        "outputId": "0fec3c41-cd24-4e27-d099-dd660f71ec8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.25.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (14.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.1)\n",
            "Requirement already satisfied: etils[enp,epath,epy,etree]>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.7.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets) (3.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2024.7.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install tensorflow\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGn30WcFSFIf"
      },
      "source": [
        "Load the presplit data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e1BMiebSFIg",
        "outputId": "ed97f3a4-abf2-45e3-cd0d-080605b7bda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-17 20:01:19--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 172.67.70.149, 104.26.3.33, 104.26.2.33, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|172.67.70.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358233 (350K) [text/tab-separated-values]\n",
            "Saving to: ‘train-data.tsv’\n",
            "\n",
            "train-data.tsv      100%[===================>] 349.84K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-07-17 20:01:20 (13.5 MB/s) - ‘train-data.tsv’ saved [358233/358233]\n",
            "\n",
            "--2024-07-17 20:01:20--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 172.67.70.149, 104.26.3.33, 104.26.2.33, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|172.67.70.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118774 (116K) [text/tab-separated-values]\n",
            "Saving to: ‘valid-data.tsv’\n",
            "\n",
            "valid-data.tsv      100%[===================>] 115.99K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-07-17 20:01:20 (8.75 MB/s) - ‘valid-data.tsv’ saved [118774/118774]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34PUuUbSFIg"
      },
      "source": [
        "Next we load the data into a data frame and also label our columns, Class and Message. Then visulaize the data with .head() and .info() methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3pQo2DmQSFIg"
      },
      "outputs": [],
      "source": [
        "# Import the training data from a tab-separated file\n",
        "# The file is read into a pandas DataFrame with no header row\n",
        "# The columns are named 'y' (label) and 'x' (message)\n",
        "dfTrain = pd.read_csv(train_file_path, sep=\"\\t\", header=None, names=['Class', 'Message'])\n",
        "\n",
        "# Display the first 5 rows of the DataFrame to inspect its structure and content\n",
        "#dfTrain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgBN-TObSFIh",
        "outputId": "2cb12d81-42ff-422f-970e-e80d87054600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4179 entries, 0 to 4178\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Class    4179 non-null   object\n",
            " 1   Message  4179 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 65.4+ KB\n"
          ]
        }
      ],
      "source": [
        "dfTrain.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ2AKdM-SFIh"
      },
      "source": [
        "The same thing is done with the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CWCxjt0zSFIi"
      },
      "outputs": [],
      "source": [
        "# Import the training data from a tab-separated file\n",
        "# The file is read into a pandas DataFrame with no header row\n",
        "# The columns are named 'y' (label) and 'x' (message)\n",
        "dfTest = pd.read_csv(test_file_path, sep=\"\\t\", header=None, names=['Class', 'Message'])\n",
        "# Display the first 5 rows of the DataFrame to inspect its structure and content\n",
        "#dfTest.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spDwqhcKSFIi"
      },
      "source": [
        "Knowing the sizes of these datasets is useful for understanding how much data we have available for training our model and how much data will be used for evaluating its performance, so next we gonna print out the number of rows of each dataframe, to know just how much we are dealing with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn7HmjTgSFIi",
        "outputId": "7d812847-b6de-4f7a-f316-91b78ebc41e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4179\n",
            "1392\n"
          ]
        }
      ],
      "source": [
        "# Print the number of rows (examples) in the training dataset\n",
        "print(len(dfTrain))\n",
        "\n",
        "# Print the number of rows (examples) in the testing dataset\n",
        "print(len(dfTest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdCIpaWPSFIi"
      },
      "source": [
        ">Remember **Ham = Normal Message**, **Spam is a SCAM MESSAGE**\n",
        "\n",
        "Next we move on to prepare the **training and testing data for a classification task where the goal is to classify SMS messages into \"ham\" (0) or \"spam\" (1)**. this is how we want our data to be transformed.\n",
        "\n",
        "**train_message will contain:** ['Hello, how are you?', 'Win money now!', 'Are you coming to the party?', 'You've won a free gift card!', 'Let's catch up soon.']\n",
        "   \n",
        "**train_label will be:** **[0, 1, 0, 1, 0]** (converted from \"ham\" and \"spam\" to 0 and 1 respectively)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FjlHfh9MSFIj"
      },
      "outputs": [],
      "source": [
        "# Rememeber train_file and test_file are DataFrames with columns 'class' and 'message'\n",
        "# and train_file and test_file have already been defined and loaded from TSV files.\n",
        "\n",
        "# Extract messages and labels from the training data\n",
        "train_message = dfTrain[\"Message\"].values.tolist()  # Convert the 'message' column to a list\n",
        "train_label = np.array([0 if x == \"ham\" else 1 for x in dfTrain['Class'].values.tolist()])  # Create labels as binary array\n",
        "\n",
        "# Extract messages and labels from the testing data\n",
        "test_message = dfTest[\"Message\"].values.tolist()  # Convert the 'message' column to a list\n",
        "test_label = np.array([0 if x == \"ham\" else 1 for x in dfTest['Class'].values.tolist()])  # Create labels as binary array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1Iy7CNSFIj"
      },
      "source": [
        "Our data is now binary classfied, it is now either (0)ham or (1)Spam....so next am thinking of adding the words in our training data into a vocabulary dict.\n",
        "\n",
        "We are gonna build a **vocabulary dictionary** from the words present in the train_message list.\n",
        "The vocabulary dict will count each word's frequency, this can be crucial for various natural language processing tasks such as text classification or sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t9yrGEmuSFIj"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to store the vocabulary and its frequencies\n",
        "vocabulary_dict = {}\n",
        "\n",
        "# Iterate through each message in the training data\n",
        "for message in train_message:\n",
        "    # Split the message into words and iterate through each word\n",
        "    for word in message.split():\n",
        "        # Check if the word is already in the vocabulary dictionary\n",
        "        if word not in vocabulary_dict:\n",
        "            # If the word is not in the dictionary, add it with a frequency of 1\n",
        "            vocabulary_dict[word] = 1\n",
        "        else:\n",
        "            # If the word is already in the dictionary, increment its frequency by 1\n",
        "            vocabulary_dict[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imD46UfnSFIj"
      },
      "source": [
        "Good, next i want us to define two variables, Vocab Szie and Max length,These variables are fundamental for preparing text data, ensuring models handle varying message lengths appropriately and process words efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WHqPAeGpSFIk"
      },
      "outputs": [],
      "source": [
        "# Calculate the vocabulary size by determining the number of unique words in the training data vocabulary_dict\n",
        "VOCAB_SIZE = len(vocabulary_dict)\n",
        "\n",
        "# Determine the maximum length of messages in terms of word count from the training data train_message\n",
        "MAX_LENGTH = len(max(train_message, key=lambda p: len(p.split())).split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBtgQfhsSFIk"
      },
      "source": [
        "The next stop would now be to encode both our training messages and test messages, encode them into integers, remember models do not handle words, but handle numbers better. So we gonnna **Encode** and **Convert each message in train_message into a sequence of integers** using the **one_hot function** based on VOCAB_SIZE. This prepare the text data by transforming each message into a sequence of indices representing the words in a fixed-size vector space.\n",
        "\n",
        " and then we will **Pad** each encoded message sequence (encoded_train_message) to a maximum length of MAX_LENGTH. **Padding** ensures all sequences are of the same length for batch processing, essential for sequence models like RNNs or CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mK7BnorSSFIk"
      },
      "outputs": [],
      "source": [
        "# Encode each message in train_message into a sequence of integers based on VOCAB_SIZE\n",
        "encoded_train_message = [one_hot(d, VOCAB_SIZE) for d in train_message]\n",
        "\n",
        "# Pad each encoded message sequence to MAX_LENGTH to ensure uniform input size\n",
        "padded_train_message = pad_sequences(encoded_train_message, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "# Encode each message in test_message similarly to train_message\n",
        "encoded_test_message = [one_hot(d, VOCAB_SIZE) for d in test_message]\n",
        "\n",
        "# Pad each encoded test message sequence to MAX_LENGTH for consistency in input size\n",
        "padded_test_message = pad_sequences(encoded_test_message, maxlen=MAX_LENGTH, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P89GnwVbSFIl"
      },
      "source": [
        "And now we have reached my favourite part, where now we **create and build** the model. We gonna build a neural network model, we first define it as **Sequantial** where layers are added one after another, then\n",
        "\n",
        "*   We define and add the embedding layer which converts the input sequences into dense vectors of fixed size.\n",
        "\n",
        "*   Next we define and add the flatten layer, this layer flattens for us the 2D output from the embedding layer into 1D vector, making it suitable for the Dense Layer.\n",
        "\n",
        "*   We then  add the FINAL LAYER, Dense layer which Outputs a single value with a sigmoid activation function for binary classification. This is where our model will choose if the message is ham or spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X46ITMtfSFIl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model as a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer to the model\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 100, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Add a Flatten layer to flatten the input from the embedding layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a Dense layer with a single neuron and sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39lQFX9wSFIl"
      },
      "source": [
        "We now have our nn model built and stacked with layers,\n",
        "The **next step** is to **compile **the model with **adamOptimizer** used for training , **loss function** used for binary classification , and the **metric accuracy** used to evalaute the model, The purpose of this compilation is to **Configure the model** for Training. I had tought that it is also wise that we put in an **Earlystopping function**, so that it is able to **stop Training** when the **validation accuracy stops improving, preventing overfitting.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c4jNUxiwSFIm"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Define EarlyStopping to monitor validation accuracy and stop training when it stops improving\n",
        "monitor = EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=25, verbose=1, mode='max', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVzStW-DSFIm"
      },
      "source": [
        "Yeeey!!!, Our model is now built and compiled, **we are now ready to fit the model and train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqBGRyDJSFIm",
        "outputId": "db0d6bff-3413-4636-e430-d7d8edd044fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "131/131 - 5s - loss: 0.2433 - acc: 0.9122 - val_loss: 0.0983 - val_acc: 0.9777 - 5s/epoch - 36ms/step\n",
            "Epoch 2/1000\n",
            "131/131 - 4s - loss: 0.0550 - acc: 0.9861 - val_loss: 0.0568 - val_acc: 0.9813 - 4s/epoch - 29ms/step\n",
            "Epoch 3/1000\n",
            "131/131 - 3s - loss: 0.0272 - acc: 0.9935 - val_loss: 0.0458 - val_acc: 0.9871 - 3s/epoch - 26ms/step\n",
            "Epoch 4/1000\n",
            "131/131 - 3s - loss: 0.0146 - acc: 0.9959 - val_loss: 0.0436 - val_acc: 0.9885 - 3s/epoch - 20ms/step\n",
            "Epoch 5/1000\n",
            "131/131 - 2s - loss: 0.0080 - acc: 0.9986 - val_loss: 0.0420 - val_acc: 0.9871 - 2s/epoch - 17ms/step\n",
            "Epoch 6/1000\n",
            "131/131 - 3s - loss: 0.0053 - acc: 0.9995 - val_loss: 0.0471 - val_acc: 0.9856 - 3s/epoch - 21ms/step\n",
            "Epoch 7/1000\n",
            "131/131 - 3s - loss: 0.0037 - acc: 0.9998 - val_loss: 0.0469 - val_acc: 0.9864 - 3s/epoch - 22ms/step\n",
            "Epoch 8/1000\n",
            "131/131 - 2s - loss: 0.0028 - acc: 0.9998 - val_loss: 0.0488 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 9/1000\n",
            "131/131 - 2s - loss: 0.0022 - acc: 0.9998 - val_loss: 0.0448 - val_acc: 0.9864 - 2s/epoch - 16ms/step\n",
            "Epoch 10/1000\n",
            "131/131 - 2s - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0484 - val_acc: 0.9871 - 2s/epoch - 16ms/step\n",
            "Epoch 11/1000\n",
            "131/131 - 2s - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0455 - val_acc: 0.9871 - 2s/epoch - 16ms/step\n",
            "Epoch 12/1000\n",
            "131/131 - 3s - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0465 - val_acc: 0.9871 - 3s/epoch - 20ms/step\n",
            "Epoch 13/1000\n",
            "131/131 - 3s - loss: 8.3018e-04 - acc: 0.9998 - val_loss: 0.0495 - val_acc: 0.9871 - 3s/epoch - 20ms/step\n",
            "Epoch 14/1000\n",
            "131/131 - 2s - loss: 5.7779e-04 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 15/1000\n",
            "131/131 - 2s - loss: 4.9690e-04 - acc: 1.0000 - val_loss: 0.0517 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 16/1000\n",
            "131/131 - 2s - loss: 4.3419e-04 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 17/1000\n",
            "131/131 - 2s - loss: 3.2152e-04 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 18/1000\n",
            "131/131 - 2s - loss: 2.7022e-04 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9871 - 2s/epoch - 18ms/step\n",
            "Epoch 19/1000\n",
            "131/131 - 3s - loss: 2.4103e-04 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9871 - 3s/epoch - 19ms/step\n",
            "Epoch 20/1000\n",
            "131/131 - 2s - loss: 2.0215e-04 - acc: 1.0000 - val_loss: 0.0564 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 21/1000\n",
            "131/131 - 2s - loss: 1.8073e-04 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 22/1000\n",
            "131/131 - 2s - loss: 1.5532e-04 - acc: 1.0000 - val_loss: 0.0579 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 23/1000\n",
            "131/131 - 2s - loss: 1.4023e-04 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 24/1000\n",
            "131/131 - 2s - loss: 1.2309e-04 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9871 - 2s/epoch - 16ms/step\n",
            "Epoch 25/1000\n",
            "131/131 - 3s - loss: 1.1034e-04 - acc: 1.0000 - val_loss: 0.0577 - val_acc: 0.9871 - 3s/epoch - 21ms/step\n",
            "Epoch 26/1000\n",
            "131/131 - 2s - loss: 9.7769e-05 - acc: 1.0000 - val_loss: 0.0603 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 27/1000\n",
            "131/131 - 2s - loss: 8.8922e-05 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 28/1000\n",
            "131/131 - 2s - loss: 7.9801e-05 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 29/1000\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "131/131 - 2s - loss: 7.2227e-05 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9871 - 2s/epoch - 15ms/step\n",
            "Epoch 29: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e420de112d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Fit the model on the training data, with validation on the test data\n",
        "# Use EarlyStopping to prevent overfitting\n",
        "model.fit(padded_train_message, train_label, validation_data=(padded_test_message, test_label), callbacks=[monitor], epochs=1000, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F7EHMH3SFIm"
      },
      "source": [
        "Nice, we now have a fully and complete trained model, the only thing left before tests is to DEFINE a [function predict_message] that is **designed to predict whether a given SMS message is \"ham\" or \"spam\"** using the trained model. The function encodes and pads the input message, **uses the model to make a prediction**, and then maps the prediction to the corresponding class(Ham/Spam) label.\n",
        "\n",
        "*This is why we build models right?, To make USEFUL PREDICTIONS AND MORE!!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tLXSZxYkSFIn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Function to predict messages based on model\n",
        "def predict_message(pred_text):\n",
        "    class_dict = {\n",
        "        0: \"ham\",  # Map 0 to 'ham'\n",
        "        1: \"spam\"  # Map 1 to 'spam'\n",
        "    }\n",
        "\n",
        "    # Encode the input message into a sequence of integers based on VOCAB_SIZE\n",
        "    encoded_message = [one_hot(pred_text, VOCAB_SIZE)]\n",
        "\n",
        "    # Pad the encoded message sequence to MAX_LENGTH to ensure uniform input size\n",
        "    padded_message = pad_sequences(encoded_message, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    # Predict the probability of the message being 'spam'\n",
        "    prediction_prob = model.predict(padded_message)[0][0]\n",
        "\n",
        "    # Convert the probability to a percentage\n",
        "    prediction_percentage = prediction_prob * 100\n",
        "\n",
        "    # Determine the predicted class ('ham' or 'spam') based on the probability\n",
        "    predicted_class = class_dict[np.round(prediction_prob)]\n",
        "\n",
        "    # Create a nice message\n",
        "    if predicted_class == \"ham\":\n",
        "        message = f\"The message is likely 'ham' with a probability of {100 - prediction_percentage:.2f}%.\"\n",
        "    else:\n",
        "        message = f\"The message is likely 'spam' with a probability of {prediction_percentage:.2f}%.\"\n",
        "\n",
        "    # Return the probability percentage, predicted class, and the message\n",
        "    return [prediction_percentage, predicted_class, message]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4W7s3xdSFIn"
      },
      "source": [
        "Our function is defined and ready to take in messages, just as we are ready to TEST, TEST AND TEST :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO0VBJdmSFIn",
        "outputId": "f5bd6337-fc09-4e76-8503-efb2765733fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 95ms/step\n",
            "[0.13122737873345613, 'ham', \"The message is likely 'ham' with a probability of 99.87%.\"]\n"
          ]
        }
      ],
      "source": [
        "# Example input message to predict\n",
        "pred_text = \"wow, is your arm alright. that happened to me one time too\"\n",
        "\n",
        "# Get the prediction for the input message\n",
        "prediction = predict_message(pred_text)\n",
        "\n",
        "# Print the prediction result\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_NoxdOXSFIn"
      },
      "source": [
        "We have tested our model and it kicks, it works. It gives us the percentage probability and tell us if it is a ham(Not a Spam) or spam\n",
        "\n",
        "Let's run more tests, now we check if we are indeed successful or not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhJSGzF0SFIo",
        "outputId": "8495d951-ebf1-4e1f-c52a-27c14e5b8e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "The model got 6/7 predictions correct, one was wrong, it was a spam, and it classified it as Ham\n",
            " This one:sale today! to stop texts call 98912460324 \n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge, 7/7 predictions. Great job!\")\n",
        "  else:\n",
        "    print(\"The model got 6/7 predictions correct, one was wrong, it was a spam, and it classified it as Ham\\n This one:sale today! to stop texts call 98912460324 \")\n",
        "\n",
        "test_predictions()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}